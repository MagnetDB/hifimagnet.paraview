name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  # -------------------------------------------------------------------------
  # Unit tests — no ParaView, no test data required.
  # Run on every push and PR, across all supported Python versions.
  # -------------------------------------------------------------------------
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install system dependencies
        run: sudo apt-get install -y libglu1-mesa libglapi-mesa

      - name: Install dependencies
        run: pip install -e ".[test]"

      - name: Run unit tests
        # Specify the file explicitly so pytest never collects the integration
        # test modules (test_2D/3D/Axi), which import method.py and trigger a
        # top-level "from paraview.simple import …" that fails without ParaView.
        run: pytest -m unit -v --tb=short test/test_unit.py

  # -------------------------------------------------------------------------
  # Integration tests — require ParaView (headless/osmesa) and data.tar.gz.
  # Run on push to main and on manual dispatch.
  # Both the ParaView installation and the extracted test data are cached to
  # avoid redundant downloads and extraction across CI runs.
  # -------------------------------------------------------------------------
  integration-tests:
    name: Integration Tests (ParaView 5.12.0)
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'

    env:
      PV_VERSION_MAJOR: "5.12"
      PV_VERSION: "5.12.0"
      PV_FLAVOR: "osmesa-MPI"
      PY_VERSION: "3.10"
      ARCH: "x86_64"

    steps:
      - uses: actions/checkout@v4
        with:
          # Pull Git LFS objects (test/data.tar.gz, ~20 MB)
          lfs: true

      # Belt-and-suspenders: actions/checkout lfs:true sets up the filter but
      # does not always download objects on self-hosted or Gitea runners.
      # An explicit pull guarantees the real binary is present before we hash it.
      - name: Pull Git LFS objects
        run: git lfs pull

      # Cache the ParaView binary installation.
      # Key includes version and flavor so a new download is triggered only
      # when the ParaView version changes.
      - name: Cache ParaView installation
        id: cache-paraview
        uses: actions/cache@v4
        with:
          path: /opt/paraview
          key: paraview-${{ env.PV_VERSION }}-${{ env.PV_FLAVOR }}-py${{ env.PY_VERSION }}

      # System libraries are always needed (the runner is a fresh VM each run;
      # only /opt/paraview is cached, not apt packages).
      - name: Install ParaView system libraries
        run: sudo apt-get install -y libgomp1 libgl1 libglu1-mesa libglapi-mesa

      - name: Download and install ParaView (headless, osmesa)
        if: steps.cache-paraview.outputs.cache-hit != 'true'
        run: |
          mkdir -p /opt/paraview
          PV_FILE="ParaView-${PV_VERSION}-${PV_FLAVOR}-Linux-Python${PY_VERSION}-${ARCH}.tar.gz"
          PV_URL="https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v${PV_VERSION_MAJOR}&type=binary&os=Linux&downloadFile=${PV_FILE}"
          wget -qO- "${PV_URL}" | tar --strip-components=1 -xz -C /opt/paraview

      # Persist ParaView paths for all subsequent steps (pip install, pytest…).
      # Without LD_LIBRARY_PATH the ParaView C extensions cannot load their
      # shared libraries even when PYTHONPATH is correct.
      - name: Configure ParaView environment
        run: |
          echo "PYTHONPATH=/opt/paraview/lib/python${{ env.PY_VERSION }}/site-packages" >> $GITHUB_ENV
          echo "LD_LIBRARY_PATH=/opt/paraview/lib:${LD_LIBRARY_PATH}" >> $GITHUB_ENV

      # Cache the extracted test data to avoid re-extracting data.tar.gz on
      # every run. The cache key is the content hash of data.tar.gz, so a new
      # extraction is triggered only when the archive itself changes.
      - name: Cache extracted test data
        uses: actions/cache@v4
        with:
          path: |
            test/cases
            test/models
            test/Pictures
            test/.data_extracted
          key: test-data-${{ hashFiles('test/data.tar.gz') }}

      - name: Set up Python ${{ env.PY_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VERSION }}

      - name: Install Python dependencies
        run: pip install -e ".[test]"

      - name: Run integration tests
        run: pytest -m integration -v --tb=short
